{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras implementation of context2vec (Melamud et al., 2016, CoNLL)\n",
    "# bi-directional language model using CBOW and negative sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, concatenate, Flatten, dot, ReLU, Lambda, Layer\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import keras_tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 64  # following the MSCC setting for ukwac-100M corpus\n",
    "MAX_NB_WORDS = None\n",
    "MIN_WORD_FREQ = 3\n",
    "NEGATIVE_NUM = 10\n",
    "NEGATIVE_FAC = 0.25\n",
    "MIN_SENT_LEN = 10\n",
    "MAX_SENT_LEN = MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this part as a module / save the processed results for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../ukwac/ukwac_subset_1M.txt\"\n",
    "with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "    raw_text = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out too short & long sentences\n",
    "corpus = []\n",
    "for sents in raw_text:\n",
    "    sent_li = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', sents)\n",
    "    if(len(sent_li)>1):\n",
    "        for sent in sent_li:\n",
    "            sent_len = len(sent.split())            \n",
    "            if (sent_len>=MIN_SENT_LEN) & (sent_len<=MAX_SENT_LEN):\n",
    "                corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Oldham 's NHS Stop Smoking Service , established in 2001 , is here to help smokers who wish to give up .\",\n",
       " 'The support the Stop Smoking Service offer is designed to build motivation to quit , preparation for the quit day , help to survive the first week and beyond , provide useful information on keeping weight off and overcoming cravings , identifying danger zones , and coping with a relapse .',\n",
       " \"All the evidence shows that people who get support with their quit attempt are more likely to be successful than people who do n't , regardless of whether they use nicotine replacement or good-old fashioned will power .\",\n",
       " 'Stomach ulcers are made worse by smoking Smokers experience more asthma , and respiratory problems .',\n",
       " 'It can affect eyesight , bone density , and the immune system Smokers have twice as much time off work due to illnesses .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer mod\n",
    "## - <BOS> and <EOS> tokens; set freq as 0\n",
    "## - minfreq: treat all low freq words as unknown words\n",
    "## cf: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/49654\n",
    "\n",
    "def tokenizer_mod(corpus, min_freq):\n",
    "    tokenizer = Tokenizer(oov_token=\"<UNK>\", lower=True)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # low freq words to remove\n",
    "    low_freq_words = [k for k, v in tokenizer.word_counts.items() if v < min_freq]\n",
    "    for word in low_freq_words:\n",
    "        del tokenizer.word_index[word]\n",
    "        del tokenizer.word_docs[word]\n",
    "        del tokenizer.word_counts[word]\n",
    "\n",
    "    # additional word index\n",
    "    tokenizer.word_index[\"<BOS>\"] = len(tokenizer.word_index)\n",
    "    tokenizer.word_index[\"<EOS>\"] = tokenizer.word_index[\"<BOS>\"]+1\n",
    "    tokenizer.word_counts[\"<BOS>\"] = 0\n",
    "    tokenizer.word_counts[\"<EOS>\"] = 0\n",
    "    return(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_mod(corpus, MIN_WORD_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215925"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_NUM_WORDS = len(tokenizer.word_index)+1  # +1 for the padding token 0\n",
    "TOTAL_NUM_WORDS  # ref. MSCC: 100K types, ukwac_full: 160k types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\alpha for negative sampling freq smoothing\n",
    "word_sample_prob = {k:(v/TOTAL_NUM_WORDS)**NEGATIVE_FAC for k, v in tokenizer.word_counts.items()}\n",
    "temp_prob_sum = np.array(list(word_sample_prob.values())).sum()\n",
    "word_sample_prob = {k:v/temp_prob_sum for k, v in word_sample_prob.items()}\n",
    "\n",
    "def get_negative_samples(targ_word_index, no_negatives, tokenizer, word_sample_prob):\n",
    "    negative_samples = np.random.choice(list(word_sample_prob.keys()), \n",
    "                                        no_negatives, \n",
    "                                        replace=False, \n",
    "                                        p=list(word_sample_prob.values()))\n",
    "    negative_incides = [tokenizer.word_index[w] for w in negative_samples]\n",
    "    while targ_word_index in negative_incides:\n",
    "        negative_samples = np.random.choice(list(word_sample_prob.keys()), no_negatives, replace=False, p=list(word_sample_prob.values()))\n",
    "        negative_incides = [tokenizer.word_index[w] for w in negative_samples]\n",
    "    return(np.array([negative_incides]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence -> context sequences and target words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional multithreading\n",
    "import threading\n",
    "\n",
    "class threadsafe_iter:\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "\n",
    "# @threadsafe_generator # => uncomment to enable the multithreading option\n",
    "def generate_batch(tokenizer, sentences, cntx_size, batch_size):\n",
    "    # TODO: identify bottlenecks\n",
    "    # output: context sequence, targets (positive and negatives), and labels for + and - targets\n",
    "    # yield a batch for .fit_generator()\n",
    " \n",
    "    cntx_l2r = []\n",
    "    cntx_r2l = []\n",
    "    targets = []\n",
    "    targets_onehot = []\n",
    "#     labels = []\n",
    "    \n",
    "    counter = 0\n",
    "    # len(sentences) = batch size \n",
    "    ## each batch contains the list of CBOW datapoints that can created by the number of *batch_size* sentences\n",
    "    while True:\n",
    "        for sentence in sentences:\n",
    "            token_list = tokenizer.texts_to_sequences([sentence])[0] # TODO:bottleneck?\n",
    "            L = len(token_list)\n",
    "#             rdm_idx = np.random.choice(range(L), size=L, replace=False) # randomize the order of yeilded output\n",
    "            \n",
    "            for targ_idx in range(L):\n",
    "                x_l2r = [token_list[i] for i in range(0, targ_idx) if 0 <= i < L and i!=targ_idx]\n",
    "                x_r2l = [token_list[i] for i in range(targ_idx, L) if 0 <= i < L and i!=targ_idx]                \n",
    "                x_l2r = [[tokenizer.word_index[\"<BOS>\"]] + x_l2r]\n",
    "                x_r2l = [x_r2l + [tokenizer.word_index[\"<EOS>\"]]]\n",
    "                x_l2r_seq = pad_sequences(x_l2r, maxlen=cntx_size, padding='pre')[0]\n",
    "                x_r2l_seq = pad_sequences(x_r2l, maxlen=cntx_size, padding='post')[0]\n",
    "                cntx_l2r.append(x_l2r_seq)\n",
    "                cntx_r2l.append(x_r2l_seq)\n",
    "\n",
    "#                 targets.append([token_list[targ_idx]]) # for negative sampling setting\n",
    "#                 labels.append([1])\n",
    "                targets.append([token_list[targ_idx]])\n",
    "                targets_onehot.append(to_categorical(token_list[targ_idx], num_classes=len(tokenizer.word_index)+1))\n",
    "                \n",
    "#                 negatives = get_negative_samples(targ_idx, NEGATIVE_NUM, tokenizer, word_sample_prob)[0]\n",
    "#                 for negative in negatives:\n",
    "#                     cntx_l2r.append(x_l2r_seq)  # for negative sampling setting\n",
    "#                     cntx_r2l.append(x_r2l_seq)\n",
    "#                     targets.append([negative])\n",
    "#                     labels.append([0])\n",
    "\n",
    "                counter += 1\n",
    "                \n",
    "                if(counter == batch_size):\n",
    "                    # when the number of processed sentences reaches the batch num\n",
    "#                     out = ([np.asarray(targets)[rdm_idx], np.asarray(cntx_l2r)[rdm_idx], np.asarray(cntx_r2l)[rdm_idx]], \n",
    "#                            [np.asarray(labels)[rdm_idx]])\n",
    "                    out = ([np.asarray(targets), np.asarray(cntx_l2r), np.asarray(cntx_r2l)],\n",
    "                           [np.asarray(targets_onehot)])\n",
    "                    \n",
    "                    yield(out)\n",
    "                    \n",
    "                    cntx_l2r = []\n",
    "                    cntx_r2l = []\n",
    "                    targets = []\n",
    "                    targets_onehot = []\n",
    "#                     labels = []\n",
    "                    counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[9416],\n",
       "         [  23]]),\n",
       "  array([[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0, 215922],\n",
       "         [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0, 215922,   9416]],\n",
       "        dtype=int32),\n",
       "  array([[    23,   1118,    907,   2728,    121,    864,      7,    626,\n",
       "               8,    134,      5,    136,   8782,     53,    797,      5,\n",
       "             249,     52, 215923,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0],\n",
       "         [  1118,    907,   2728,    121,    864,      7,    626,      8,\n",
       "             134,      5,    136,   8782,     53,    797,      5,    249,\n",
       "              52, 215923,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0,\n",
       "               0,      0,      0,      0,      0,      0,      0,      0]],\n",
       "        dtype=int32)],\n",
       " [array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example data list from a single sentence\n",
    "next(generate_batch(tokenizer, corpus[:10], MAX_SEQUENCE_LENGTH, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 600\n",
    "H_DIM = int(DIM/2)\n",
    "D_DIM = int(DIM*2)\n",
    "O_DIM = DIM\n",
    "\n",
    "idrop = 0.3\n",
    "rdrop = 0.3\n",
    "odrop = 0.3\n",
    "\n",
    "alpha_lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "# x_targ = Input((1,))\n",
    "x_targ = Input((1,))\n",
    "x_cntx_l2r = Input((MAX_SEQUENCE_LENGTH,))\n",
    "x_cntx_r2l = Input((MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "e = Embedding(TOTAL_NUM_WORDS, DIM, name='embedding_shared')\n",
    "e_targ = e(x_targ)\n",
    "e_cntx_l2r = e(x_cntx_l2r)\n",
    "e_cntx_r2l = e(x_cntx_r2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden LSTM layers\n",
    "h_l2r = LSTM(H_DIM, go_backwards=False, dropout=idrop, recurrent_dropout=rdrop, return_sequences=False)(e_cntx_l2r) # h(<bos>) h(a) h(b)\n",
    "h_r2l = LSTM(H_DIM, go_backwards=True,  dropout=idrop, recurrent_dropout=rdrop, return_sequences=False)(e_cntx_r2l) # h(d) h(e) h(<eos>)\n",
    "h = concatenate([h_l2r, h_r2l], name='bilstm_concat')\n",
    "# h = Flatten()(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional deep layers for the biLSTM layer\n",
    "h = Dropout(odrop)(h)\n",
    "h = Dense(D_DIM, activation='linear')(h)\n",
    "h = ReLU()(h)\n",
    "h = Dropout(odrop)(h)\n",
    "h = Dense(O_DIM, activation='linear', name='bilstm_deepout')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/47892380/how-can-i-use-tensorflows-sampled-softmax-loss-function-in-a-keras-modelq\n",
    "# https://datascience.stackexchange.com/questions/28213/keras-negative-sampling-with-custom-layer\n",
    "class SampledSoftmax(Layer):\n",
    "    def __init__(self, num_sampled, num_classes, mode, **kwargs):\n",
    "        self.num_sampled = num_sampled\n",
    "        self.num_classes = num_classes\n",
    "        self.mode = mode\n",
    "        super(SampledSoftmax, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dense_shape, classes_shape = input_shape\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(self.num_classes, dense_shape[1]),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                      shape=(self.num_classes,),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)  # Maybe zero\n",
    "\n",
    "        super(SampledSoftmax, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs_and_labels):\n",
    "        inputs, labels = inputs_and_labels\n",
    "        if self.mode == \"train\":\n",
    "            loss = K.tf.nn.sampled_softmax_loss(\n",
    "                weights=self.kernel,\n",
    "                biases=self.bias,\n",
    "#                 weights = inputs._keras_history[0].weights,\n",
    "#                 bias = inputs._keras_history[0].bias,\n",
    "                labels=labels,\n",
    "                inputs=inputs,\n",
    "                num_sampled=self.num_sampled,\n",
    "                num_classes=self.num_classes,\n",
    "                num_true=1)\n",
    "\n",
    "        elif self.mode == \"eval\":\n",
    "            logits = K.tf.matmul(inputs, tf.transpose(self.kernel))\n",
    "            logits = K.tf.nn.bias_add(logits, self.bias)\n",
    "            labels_one_hot = K.tf.one_hot(labels, self.num_classes)\n",
    "            loss = K.tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=labels_one_hot,\n",
    "                logits=logits)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        dense_shape, classes_shape = input_shape\n",
    "        return (dense_shape[0], )\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SampledSoftmax, self).get_config()\n",
    "        config['num_sampled'] = self.num_sampled\n",
    "        config['num_classes'] = self.num_classes\n",
    "        config['mode'] = self.mode\n",
    "        return config\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = {'num_sampled':self.num_sampled, 'num_classes':self.num_classes, 'mode':self.mode}\n",
    "#         base_config = super(SampledSoftmax, self).get_config()\n",
    "#         return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sjnam/anaconda3/envs/infmtv_keras_gpu/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "# # negative sampling sigmoids\n",
    "# targ_cntx = dot([e_targ, h], axes=-1, normalize=False)\n",
    "# targ_cntx = Dense(1, activation='sigmoid', name='target_sigmoid')(targ_cntx)\n",
    "# targ_cntx = Flatten()(targ_cntx)\n",
    "# targ_cntx = Lambda(lambda x:K.sum(x, axis=2), name='target_loss')(targ_cntx)\n",
    "\n",
    "# negative sampled softmax\n",
    "# targ_cntx = Dense(NEGATIVE_NUM+1, activation='softmax')(h)\n",
    "targ_cntx = SampledSoftmax(NEGATIVE_NUM, TOTAL_NUM_WORDS, mode='train', name='sampled_softmax')([h, x_targ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_shared (Embedding)    multiple             129555000   input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          1081200     embedding_shared[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 300)          1081200     embedding_shared[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bilstm_concat (Concatenate)     (None, 600)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           bilstm_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1200)         721200      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 1200)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1200)         0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bilstm_deepout (Dense)          (None, 600)          720600      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sampled_softmax (SampledSoftmax (None,)              129770925   bilstm_deepout[0][0]             \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 262,930,125\n",
      "Trainable params: 262,930,125\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model compile\n",
    "# model = Model([x_targ, x_cntx_l2r, x_cntx_r2l], targ_cntx) \n",
    "model = Model([x_targ, x_cntx_l2r, x_cntx_r2l], [targ_cntx]) \n",
    "adam = optimizers.Adam(lr=alpha_lr)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
    "# model.compile(loss=custom_loss, optimizer=adam)\n",
    "model.compile(loss=lambda y_true, loss:loss, optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3415481 3415481 34154 0 3\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(corpus)\n",
    "# train_idx = len(corpus) - round(len(corpus)*VALIDATION_SPLIT)\n",
    "train_idx = corpus_size\n",
    "batch_size = 100\n",
    "n_steps_train = (train_idx//batch_size)\n",
    "n_steps_val = ((len(corpus)-train_idx)//batch_size)\n",
    "num_iter = 3\n",
    "print(corpus_size, train_idx, n_steps_train, n_steps_val, num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjnam/anaconda3/envs/infmtv_keras_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 129555000 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n",
      "/home/sjnam/anaconda3/envs/infmtv_keras_gpu/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca70c3e34194718b95e287e9b12a24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=3, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8b806bd72a4bf0934d6ea05a076859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=34154, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cf8e9439ff4b5d9190c1770daf405a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=34154, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e122cf30e84230a63352ff969fc56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=34154, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2e48e71e80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generate_batch(tokenizer, corpus, MAX_SEQUENCE_LENGTH, batch_size), \n",
    "                    steps_per_epoch=n_steps_train, epochs=num_iter,\n",
    "#                     validation_data=generate_batch(tokenizer, corpus[train_idx:], MAX_SEQUENCE_LENGTH, batch_size), \n",
    "#                     validation_steps=n_steps_val,\n",
    "                    verbose=0, callbacks=[keras_tqdm.TQDMNotebookCallback(leave_inner=True, leave_outer=True)],\n",
    "                    max_queue_size=16, \n",
    "                    workers=8, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jovianlin.io/saving-loading-keras-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cbow_bilstm_ukwac_100M_a_'+str(alpha_lr)+'_dim_'+str(DIM)+'_nfac_'+str(NEGATIVE_FAC)+ \\\n",
    "           '_drop_'+str(odrop)+'_epochs_'+str(num_iter)+'neg.h5')\n",
    "\n",
    "with open('cbow_bilstm_ukwac_100M_a_'+str(alpha_lr)+'_dim_'+str(DIM)+'_nfac_'+str(NEGATIVE_FAC)+ \\\n",
    "           '_drop_'+str(odrop)+'_epochs_'+str(num_iter)+'neg.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cannot load the custom later \n",
    "# model = load_model('cbow_bilstm_ukwac_1M_a_'+str(alpha_lr)+'_dim_'+str(DIM)+'_nfac_'+str(NEGATIVE_FAC)+ \\\n",
    "#                    '_drop_'+str(odrop)+'_epochs_'+str(num_iter)+'neg.h5', \n",
    "#                    custom_objects={'SampledSoftmax':SampledSoftmax(NEGATIVE_NUM, TOTAL_NUM_WORDS, 'eval')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import model_from_json\n",
    "# with open('cbow_bilstm_ukwac_100M_a_'+str(alpha_lr)+'_dim_'+str(DIM)+'_nfac_'+str(NEGATIVE_FAC)+ \\\n",
    "#            '_drop_'+str(odrop)+'_epochs_'+str(num_iter)+'neg.json', 'r') as f:\n",
    "#     model = model_from_json(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('cbow_bilstm_ukwac_100M_a_'+str(alpha_lr)+'_dim_'+str(DIM)+'_nfac_'+str(NEGATIVE_FAC)+ \\\n",
    "                   '_drop_'+str(odrop)+'_epochs_'+str(num_iter)+'neg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_1:0' shape=(?, 1) dtype=float32>,\n",
       " <tf.Tensor 'input_2:0' shape=(?, 64) dtype=float32>,\n",
       " <tf.Tensor 'input_3:0' shape=(?, 64) dtype=float32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing sentence vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h1 = Model(model.input[1:], model.get_layer('bilstm_concat').output)\n",
    "model_h2 = Model(model.input[1:], model.get_layer('bilstm_deepout').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec1_h1 = model_h1.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> A bunch of kids broken in and\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" the paintings <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec1_h2 = model_h2.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> A bunch of kids broken in and\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" the paintings <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec2_h1 = model_h1.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> A bunch of kids broken in and\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" the books <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec2_h2 = model_h2.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> A bunch of kids broken in and\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" the books <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec3_h1 = model_h1.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> This is not a good\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" for her <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec3_h2 = model_h2.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> This is not a good\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" for her <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec4_h1 = model_h1.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> This \"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" is due not just to mere luck <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vec4_h2 = model_h2.predict([pad_sequences(tokenizer.texts_to_sequences([\"<BOS> This \"]), maxlen=MAX_SEQUENCE_LENGTH, padding='pre'), \n",
    "                                 pad_sequences(tokenizer.texts_to_sequences([\" is due not just to mere luck <EOS>\"]), maxlen=MAX_SEQUENCE_LENGTH, padding='post')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.01980934,  0.        ,  0.        , -0.        , -0.10852313],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec1_h1[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03524098,  0.01960367,  0.04008206, -0.03422608, -0.02447513,\n",
       "        0.08366579,  0.06117012, -0.03808985,  0.03539947,  0.06514335],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec1_h2[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.01980934,  0.        ,  0.        , -0.        , -0.10852313],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec2_h1[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02010383,  0.02865812,  0.03589398, -0.036089  , -0.03837456,\n",
       "        0.07460199,  0.0405308 , -0.02475565,  0.04971936,  0.05991247],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec2_h2[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.31516954,  0.        ,  0.        , -0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec3_h1[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14028482, -0.04478314,  0.03191046, -0.10079529, -0.2970091 ,\n",
       "        0.01752447,  0.00672428,  0.1021916 ,  0.08007528,  0.07870147],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec3_h2[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.00494273],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec4_h1[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.5300161e-02,  5.7356510e-02, -5.4492977e-02, -9.4825868e-05,\n",
       "       -1.3343853e-01,  5.6117371e-02,  4.1513979e-02, -3.0547578e-02,\n",
       "        4.8666321e-02,  1.0632966e-02], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vec4_h2[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911181926727295"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sent_vec1_h2, sent_vec2_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3632470667362213"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sent_vec1_h2, sent_vec3_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37099379301071167"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sent_vec2_h2, sent_vec3_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.561137318611145"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sent_vec3_h2, sent_vec4_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inferred fillers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_word = model.get_layer('embedding_shared').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20017202198505402"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-cosine(vectors_word[tokenizer.word_index[\"she\"]], vectors_word[tokenizer.word_index[\"her\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09070437401533127"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-cosine(vectors_word[tokenizer.word_index[\"she\"]], vectors_word[tokenizer.word_index[\"box\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15406030416488647"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-cosine(vectors_word[tokenizer.word_index[\"school\"]], vectors_word[tokenizer.word_index[\"book\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036152709275484085"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-cosine(vectors_word[tokenizer.word_index[\"school\"]], vectors_word[tokenizer.word_index[\"tiger\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1985,  455, 1297, ..., 1824, 1722, 1180])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # def sim_mult(sent):\n",
    "sent_vec = sent_vec3_h2[0]\n",
    "sent_vec = sent_vec / np.sqrt((sent_vec*sent_vec).sum())\n",
    "\n",
    "# targ_sim = vectors_word.dot(vectors_word[tokenizer.word_index[\"<UNK>\"]])\n",
    "# targ_sim[targ_sim<0] = 0.0\n",
    "# cntx_sim = vectors_word.dot(sent_vec)\n",
    "# cntx_sim[cntx_sim<0] = 0.0\n",
    "# mult_sim = targ_sim * cntx_sim\n",
    "\n",
    "mult_sim = (vectors_word.dot(sent_vec)+1.0)/2\n",
    "\n",
    "(-mult_sim).argsort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed:0.9797227382659912\n",
      "st:0.884330153465271\n",
      "policies:0.8756725788116455\n",
      "sport:0.8592591881752014\n",
      "problems:0.8470306992530823\n",
      "although:0.8468138575553894\n",
      "though:0.8395321369171143\n",
      "mean:0.8349412679672241\n",
      "deals:0.82281494140625\n",
      "suggests:0.8119405508041382\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in (-mult_sim).argsort():\n",
    "    if np.isnan(mult_sim[i]):\n",
    "        continue\n",
    "    if (i != 0):\n",
    "        print('{0}:{1}'.format(tokenizer.index_word[i], mult_sim[i]))\n",
    "        count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
